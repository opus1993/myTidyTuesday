---
title: "Sliced NYC Airbnb"
author: "Jim Gruman"
date: "June 29, 2021"
output:
  workflowr::wflow_html:
    toc: no
    code_folding: hide
    code_download: true
    df_print: paged
editor_options:
  chunk_output_type: console
---

[SLICED](https://www.notion.so/SLICED-Show-c7bd26356e3a42279e2dfbafb0480073) is like the TV Show Chopped but for data science. Competitors get a never-before-seen dataset and two-hours to code a solution to a prediction challenge. Contestants get points for the best model plus bonus points for data visualization, votes from the audience, and more.
 
[Season 1 Episode 5](https://www.kaggle.com/c/sliced-s01e05-WXx7h8/data) featured a challenge to predict AirBnb pricing for properties in New York City. The evaluation metric in this competition is residual mean log squared error.

![](https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F7f7ba5f9-d7bd-4101-8933-a112b4f78570%2FFrame_3.png?table=block&id=c7bd2635-6e3a-4227-9e2d-fbafb0480073&spaceId=2cc404e6-fe20-483d-9ea5-5d44eb3dd586&width=1510&userId=&cache=v2)

To make the best use of the resources that we have, we will explore the data set features to select those with the most predictive power, build a random forest to confirm the recipe, and then use the rest of the time to build one or more catboost ensemble models. 
Let's load up some packages:

```{r setup}

suppressPackageStartupMessages({
library(tidyverse)
library(hrbrthemes)
library(lubridate)
library(tidytext)

library(tidymodels)
library(textrecipes)
library(treesnip)
library(finetune)
library(stacks)
library(themis)
library(baguette)
  
library(catboost)

})

source(here::here("code","_common.R"),
       verbose = FALSE,
       local = knitr::knit_global())

ggplot2::theme_set(theme_jim(base_size = 12))

#create a data directory
data_dir <- here::here("data",Sys.Date())
if (!file.exists(data_dir)) dir.create(data_dir)

# set a competition metric
mset <- metric_set(mn_log_loss)

# set the competition name from the web address
competition_name <- "sliced-s01e05-WXx7h8"

zipfile <- paste0(data_dir,"/", competition_name, ".zip")

path_export <- here::here("data",Sys.Date(),paste0(competition_name,".csv"))
```

A quick reminder before downloading the dataset:  Go to the web site and accept the competition terms!!!

# Import and EDA {.tabset}

I was out on holiday this past week and missed the live competition. This file is a compilation of some of the best parts of what the contestants demonstrated (live coding from scratch) and Julia Silge's [custom metric blog post](https://juliasilge.com/blog/nyc-airbnb/).

## Direct Import and Skim

We have basic shell commands available to interact with Kaggle here:

```{r kaggle competitions terminal commands, eval=FALSE}
# from the Kaggle api https://github.com/Kaggle/kaggle-api

# the leaderboard
shell(glue::glue('kaggle competitions leaderboard { competition_name } -s'))

# the files to download
shell(glue::glue('kaggle competitions files -c { competition_name }'))

# the command to download files
shell(glue::glue('kaggle competitions download -c { competition_name } -p { data_dir }'))

# unzip the files received
shell(glue::glue('unzip { zipfile } -d { data_dir }'))

```

Reading in the contents of the datafiles here:

```{r}
train_df <-
  read_csv(file = glue::glue({
    data_dir
  }, "/train.csv")) %>%
  mutate(across(c(id, host_id), as.character)) %>%
  mutate(across(
    c(host_name, neighbourhood_group, neighbourhood, room_type),
    as.factor
  )) %>%
  mutate(price = log10(price + 1))

test_df <-
  read_csv(file = glue::glue({
    data_dir
  }, "/test.csv")) %>%
  mutate(across(c(id, host_id), as.character)) %>%
  mutate_if(is.character, as.factor) 

```

Some questions to answer here:
What features have missing data, and imputations may be required?
What does the outcome variable look like, in terms of imbalance?

```{r, eval=FALSE}
skimr::skim(train_df)
```

Outcome variable `log10(price)` has a mean of 2.06 and a range between 0 and 4. Numerical feature `reviews_per_month` and date `last_review` are missing about 20% of the time in training data. Categorical variable `neighbourhood` has 217 levels.

## Categorical Feature Plots

```{r, fig.asp=1.2}
summarize_prices <- function(tbl){
  tbl %>% 
    summarize(median_price = 10^median(price) - 1,
              n = n(),
              mean_price = 10^mean(price) - 1) %>% 
    arrange(desc(n))
}

train_df %>%
  group_by(neighbourhood_group = withfreq(neighbourhood_group)) %>%
  ggplot(aes(10 ^ price, 
             neighbourhood_group, 
             color = neighbourhood_group)) +
  ggdist::stat_dots(
    aes(fill = neighbourhood_group),
    side = "top",
    alpha = 0.2,
    justification = -0.1,
    binwidth = 0.03,
    dotsize = .5,
    stackratio = .1,
    normalize = "groups",
    show.legend = FALSE
  ) +
  geom_boxplot(
    width = 0.1,
    outlier.shape = NA,
    show.legend = FALSE
  ) +
  scale_x_log10(labels = scales::dollar_format(accuracy = 1)) +
  labs(y = NULL, x = "Price", title = "NYC Airbnb by Neighborhood Groups") +
  theme(panel.grid.major.y = element_blank())

train_df %>%
  mutate(host_id = fct_lump(host_id, 30),
         host_id = fct_reorder(host_id, price)) %>%
  group_by(host_id = withfreq(host_id)) %>%
  ggplot(aes(10 ^ price , host_id)) +
  geom_boxplot(show.legend = FALSE) +
  scale_x_log10(labels = scales::dollar_format(accuracy = 1)) +
  labs(y = NULL, x = "Price", title = "NYC Airbnb by Host IDs")

train_df %>%
  mutate(room_type = fct_reorder(room_type, price)) %>%
  group_by(room_type) %>%
  ggplot(aes(10 ^ price, 
             room_type,
             color = room_type)) +
  ggdist::stat_dots(
    aes(fill = room_type),
    side = "top",
    alpha = 0.2,
    justification = -0.1,
    binwidth = 0.03,
    dotsize = .5,
    stackratio = .08,
    normalize = "groups",
    show.legend = FALSE
  ) +
  geom_boxplot(
    width = 0.1,
    outlier.shape = NA,
    show.legend = FALSE
  ) +
  scale_x_log10(labels = scales::dollar_format(accuracy = 1)) +
  labs(y = NULL, x = "Price", title = "NYC Airbnb by Room Type")

train_df %>%
  unnest_tokens(word, name) %>%
  anti_join(stop_words) %>% 
  group_by(word ) %>%
  summarize_prices() %>%
  head(30) %>%
  mutate(word = fct_reorder(word, mean_price)) %>%
  ggplot(aes(mean_price, word, size = n)) +
  geom_point() +
  scale_x_log10(labels = scales::dollar_format(accuracy = 1)) +
  labs(size = "Number of Listings", title = "NYC AirBNB Property Description NLP", y = NULL) +
  theme(legend.position = c(0.8, 0.3),
        legend.background = element_rect(color = "white"))

train_df %>% 
  group_by(neighbourhood) %>% 
  summarize_prices() %>% 
  arrange(-median_price) 

```

## Numeric Feature Plots

The outcome variable itself is skewed across all observations in the training data, as prices often are.

```{r}
train_df %>%
  ggplot(aes(10^price + 1, fill = neighbourhood_group)) +
  geom_histogram(position = "identity", 
                 alpha = 0.5, 
                 bins = 40) +
  scale_x_log10(labels = scales::dollar_format(accuracy = 1),
                breaks = c(5, 10, 100, 1000)) +
  labs(fill = NULL, x = "price per night",
       title = "NYC Airbnb") +
  theme(legend.position = c(0.8, 0.5),
        legend.background = element_rect(color = "white"))
```

Let's explore the relationships between the features and the numeric outcome. 

A plot of the time series `last_review` by year and the corresponding ranges of price

```{r}
train_df %>%
  group_by(year = year(last_review)) %>%
  summarize(mean_price = mean(10^price - 1),
            n = n(),
            low = quantile(10^price - 1, probs = 0.25, na.rm = TRUE),
            high = quantile(10^price - 1, probs = 0.75, na.rm = TRUE),
              .groups = "drop") %>%
  filter(!is.na(year)) %>% 
  ggplot(aes(year, mean_price)) +
  geom_line() +
  geom_point(aes(size = n)) +
  geom_ribbon(aes(ymin = low, ymax = high), alpha = .1) +
  expand_limits(y = 0) +
  scale_x_continuous(breaks = seq(2011, 2017, 2)) +
  scale_y_continuous(labels = scales::dollar_format(accuracy = 1)) +
  labs(x = "Year", y = "Mean Price and IQR",
       size = "Reviews", title = "NYC Airbnb") +
  theme(legend.position = c(0.85, 0.2),
        legend.background = element_rect(color = "white"))
```

Histograms of the distributions of each numeric feature:

```{r numeric_feature1, fig.asp=1}
train_numeric <- train_df %>% keep(is.numeric) %>% colnames()

chart <- c(train_numeric)

train_df %>%
  select_at(vars(all_of(chart))) %>%
  select(-price) %>% 
  pivot_longer(cols = everything(),
    names_to = "key",
    values_to = "value"
  ) %>% 
  filter(!is.na(value)) %>% 
  ggplot() +
  geom_histogram(mapping = aes(x = value, fill = key),
                 bins = 50, show.legend = FALSE) +
  facet_wrap(~ key, scales = "free", ncol = 3) +
  labs(title = "NYC Airbnb Numeric Feature Histograms")

```

And the outcome variable by each numeric feature:

```{r}
train_df %>%
  select_at(vars(all_of(chart))) %>%
  pivot_longer(cols = -price,
               names_to = "key",
               values_to = "value") %>%
  filter(!is.na(value)) %>%
  ggplot() +
  geom_point(
    mapping = aes(x = value, y = 10 ^ price + 1),
    alpha = 0.1,
    shape = 20,
    show.legend = FALSE
  ) +
  geom_smooth(aes(
    x = value,
    y = 10 ^ price + 1 ,
    color = key),
    method = "gam", show.legend = FALSE) +
  facet_wrap(~ key, scales = "free", ncol = 3) +
  scale_y_log10(labels = scales::dollar_format(accuracy = 1)) +
  labs(x = NULL, y = "Price", title = "NYC Airbnb")
```

## Map of prices 

Adapted from Julia Silge's demonstration

```{r, fig.asp=1}
usa_data = map_data("usa")

train_df %>%
  ggplot(aes(longitude, latitude, z = price)) +
  stat_summary_hex(bins = 100) +
  labs(fill = "Mean price(log10)") +
  cowplot::theme_map() +
  labs(title = "NYC Airbnb Price Ranges")
  theme(legend.position = c(0.1, 0.8),
        legend.background = element_rect(color = "white"))
```

## Numeric Feature Correlations

```{r corrplot, fig.asp=1}
cr1 <- train_df %>%
  select(-price) %>%
  keep(is.numeric) %>%
  cor(use = "pair")

corrplot::corrplot(cr1, type = "upper")

```

# {-}

Conclusions:

* Neighborhood location drives price, through both named neighborhood and through longitude and latitude.
* The listing names have some interesting words.
* Some of the hosts draw higher pricing.
* the numeric features do not provide as much predictive power alone
* there are AirBNB listings that have neither reviews nor last_review dates. 

# Preprocessiong {.tabset}

## The recipe framework

I am going to train on almost all of the train data. The 1% left is a quick confirmation of validity.  The provided file labeled `test` has no labels.

```{r}
set.seed(2021)
split <- train_df %>% 
  initial_split(strata = price,
                       prop = .99)
train <- training(split)
valid <- testing(split)
```

There are only `r nrow(valid)` held out from training as a last check before submission.

```{r}
basic_rec <-
  recipe(price ~ ., train) %>%
  # ---- set aside the row id's
  update_role(id, host_name, new_role = "id") %>% 
  step_novel(neighbourhood) %>%
  step_other(neighbourhood, threshold = 0.01) %>%
  step_novel(host_id) %>%
  step_other(host_id, threshold = 0.01) %>%
  step_tokenize(name) %>%
  step_stopwords(name) %>%
  step_texthash(name, num_terms = 16) %>% 
  step_indicate_na(last_review, reviews_per_month) %>% 
  step_mutate(last_review = if_else(is.na(last_review),
                                    min(train$last_review, na.rm = TRUE), last_review)) %>% 
  step_holiday(last_review) %>% 
  step_date(last_review, features = "year",
            keep_original_cols = FALSE) %>% 
  step_ns(longitude, latitude, deg_free = 4) %>%
  step_normalize(all_numeric_predictors()) %>% 
  step_nzv(all_predictors())
    
```

## the pre-processed data

```{r}
basic_rec %>% 
#  finalize_recipe(list(num_comp = 2)) %>% 
  prep() %>% 
  juice() 
```

## Cross Validation

We will use 5-fold cross validation and stratify between the rain and no-rain classes.

```{r}
train_folds <- vfold_cv(data = train,
                        strata = price,
                        v = 5)

```

# {-}

# Machine Learning {.tabset}

## Model Specifications

We will build a specification for simple shallow random forest and a specification for catboost. 

```{r}
catboost_spec <- boost_tree(trees = 1000,
                            min_n = tune(),
                            learn_rate = tune(),
                            tree_depth = tune()) %>% 
  set_engine("catboost") %>%
  set_mode("regression")

bag_spec <-
  bag_tree(min_n = 10) %>%
  set_engine("rpart", times = 50) %>%
  set_mode("regression")

```

## The RMSLE custom metric

The yardstick default RMSE is on the log of price, not RMSLE on price. This will create a custom function to track RMSLE:

```{r}
library(rlang)

rmsle_vec <- function(truth, estimate, na_rm = TRUE, ...) {
  rmsle_impl <- function(truth, estimate) {
    sqrt(mean((log(truth + 1) - log(estimate + 1))^2))
  }

  metric_vec_template(
    metric_impl = rmsle_impl,
    truth = truth,
    estimate = estimate,
    na_rm = na_rm,
    cls = "numeric",
    ...
  )
}

rmsle <- function(data, ...) {
  UseMethod("rmsle")
}

rmsle <- new_numeric_metric(rmsle, direction = "minimize")

rmsle.data.frame <- function(data, truth, estimate, na_rm = TRUE, ...) {
  metric_summarizer(
    metric_nm = "rmsle",
    metric_fn = rmsle_vec,
    data = data,
    truth = !!enquo(truth),
    estimate = !!enquo(estimate),
    na_rm = na_rm,
    ...
  )
}

mset = metric_set(rmsle)
```

## Parallel backend

To speed up computation we will use a parallel backend.

```{r}
all_cores <- parallelly::availableCores(omit = 1)
all_cores

future::plan("multisession", workers = all_cores) # on Windows

```

## A quick Random Forest

Lets make a cursory check of the recipe and variable importance, which comes out of `rpart` for free. This workflow also handles factors without dummies.

```{r random_forest_fit, fig.asp=1}
bag_wf <-
  workflow() %>%
  add_recipe(basic_rec) %>%
  add_model(bag_spec)

set.seed(123)
bag_fit <- parsnip::fit(bag_wf, data = train)

extract_fit_parsnip(bag_fit)$fit$imp %>%
  mutate(term = fct_reorder(term, value)) %>%
  ggplot(aes(value, term)) +
  geom_point() +
  geom_errorbarh(aes(
    xmin = value - `std.error` / 2,
    xmax = value + `std.error` / 2
  ),
  height = .3) +
  labs(title = "Feature Importance",
       x = NULL, y = NULL)

```

We see that `room_type` and the geographical information will be very important for this model.

## Tuning Catboost

Now that we have some confidence that the features have predictive power, lets tune up a set of `catboost` models. 

```{r catboost_tuning_params}

catboost_params <-
  dials::parameters(min_n(), # min data in leaf
                    tree_depth(range = c(4, 15)),
                    learn_rate(range = c(-3, -0.7), 
                               trans = log10_trans())
                    )
                    
cbst_grid <- dials::grid_max_entropy(catboost_params,
                                     size = 40 
                                     )
cbst_grid
```

```{r catboost_tuning_no_eval, eval=FALSE}
cv_res_catboost <-
  workflow() %>% 
  add_recipe(basic_rec) %>% 
  add_model(catboost_spec) %>% 
  tune_grid(    
    resamples = train_folds,
    grid = cbst_grid,
    control = control_race(verbose = FALSE,
                           save_pred = TRUE, 
                           save_workflow = TRUE,
                           extract = extract_model,
                           parallel_over = "resamples"),
    metrics = mset
)
```

```{r catboost_tuning_no_include, include=FALSE}
if (file.exists(here::here("data","airbnbcatboost.rds"))) {

cv_res_catboost <- read_rds(here::here("data","airbnbcatboost.rds"))
  
} else {  
cv_res_catboost <-
  workflow() %>% 
  add_recipe(basic_rec) %>% 
  add_model(catboost_spec) %>% 
  tune_grid(    
    resamples = train_folds,
    grid = cbst_grid,
    control = control_race(verbose = FALSE,
                           save_pred = TRUE, 
                           save_workflow = TRUE,
                           extract = extract_model,
                           parallel_over = "resamples"),
    metrics = mset
)

write_rds(cv_res_catboost, here::here("data","airbnbcatboost.rds"))
}
```

```{r catboost_tuning_performance}
autoplot(cv_res_catboost)

show_best(cv_res_catboost) %>% 
  select(-.estimator)

cat_wf_best <-   
  workflow() %>% 
  add_recipe(basic_rec) %>% 
  add_model(catboost_spec) %>% 
  finalize_workflow(select_best(cv_res_catboost))

cat_fit_best <- cat_wf_best %>%
  parsnip::fit(data = train)
```

# {-}

Catboost model performance

```{r}
predict(cat_fit_best, new_data = valid) %>%
  cbind(valid) %>%
  ggplot(aes(10^price + 1, 
             10^.pred + 1, 
             color = neighbourhood_group)) +
  geom_abline(
    slope = 1,
    lty = 2,
    color = "gray50",
    alpha = 0.5
  ) +
  geom_point(alpha = 0.8, shape = 21) +
  scale_x_log10(labels = scales::dollar_format(accuracy = 1)) +
  scale_y_log10(labels = scales::dollar_format(accuracy = 1)) +
  labs(color = NULL, 
       x = "True price", 
       y = "Predicted price")

predict(cat_fit_best, new_data = valid) %>%
  cbind(valid) %>%
  rmsle(10^price - 1, 10^.pred - 1)

```

This catboost figure is somewhat better than the leaderboard RMSLE of 0.40758, so it would have been worthy of submission.

Lastly, we build the submission file 

```{r, eval=FALSE}

bind_cols(predict(cat_fit_best, test_df), test_df) %>% 
  select(id, price = .pred) %>%
  write_csv(file = path_export)

```

and make the submission to the Kaggle board

```{r, eval = FALSE}
shell(glue::glue('kaggle competitions submit -c { competition_name } -f { path_export } -m "Catboosted"'))
```



