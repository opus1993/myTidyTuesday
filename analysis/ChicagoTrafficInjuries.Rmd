---
title: "Chicago Traffic Injuries"
author: "Jim Gruman"
date: "January 5, 2021"
output:
  workflowr::wflow_html:
    toc: no
    code_folding: hide
editor_options:
  chunk_output_type: console
---

> Inspired by Julia Silge's [Predicting injuries for Chicago traffic crashes](https://juliasilge.com/blog/chicago-traffic-model/)

Our goal here is to demonstrate how to use the `tidymodels` framework to model live-caught data on [traffic crashes in the City of Chicago](https://data.cityofchicago.org/Transportation/Traffic-Crashes-Crashes/85ca-t3if) on injuries.

```{r setup}

suppressPackageStartupMessages({
library(tidyverse)
library(lubridate)
library(here)
library(tidymodels)
library(RSocrata)
library(themis)        # upsample/downsample for unbalanced datasets
library(baguette)      # bagging models
})

source(here::here("code","_common.R"),
       verbose = FALSE,
       local = knitr::knit_global())

ggplot2::theme_set(theme_jim(base_size = 12))
```

We will load the latest data directly from the Chicago data portal. This dataset covers traffic crashes on city streets within Chicago city limits under the jurisdiction of the Chicago Police Department.

Let’s download the last three years of data to train our model.

```{r}
years_ago <- today() - years(3)
crash_url <- glue::glue("https://data.cityofchicago.org/Transportation/Traffic-Crashes-Crashes/85ca-t3if?$where=CRASH_DATE > '{years_ago}'")
crash_raw <- as_tibble(read.socrata(crash_url))
```

Data preparation

```{r}
crash <- crash_raw %>%
  arrange(desc(crash_date)) %>%
  transmute(
    injuries = if_else(injuries_total > 0, "injuries", "noninjuries"),
    crash_date,
    crash_hour,
    report_type = if_else(report_type == "", "UNKNOWN", report_type),
    num_units,
    trafficway_type,
    posted_speed_limit,
    weather_condition,
    lighting_condition,
    roadway_surface_cond,
    first_crash_type,
    trafficway_type,
    prim_contributory_cause,
    latitude, longitude
  ) %>%
  na.omit()
```

```{r timeSeries, warning=FALSE}
crash %>%
  mutate(crash_date = as_date(floor_date(crash_date, unit = "week"))) %>%
  count(crash_date, injuries) %>%
  filter(
    crash_date != last(crash_date),
    crash_date != first(crash_date)
  ) %>%
  mutate(name_lab = if_else(crash_date == last(crash_date), injuries, NA_character_)) %>% 
  ggplot() +
  geom_line(aes(as.Date(crash_date), n, color = injuries),
            size = 1.5, alpha = 0.7) +
  scale_x_date(labels = scales::date_format("%Y"),
                   expand = c(0, 0),
                   breaks = seq.Date(as_date("2018-09-01"),
                                     as_date("2021-09-01"),
                                     by = "year"),
                   minor_breaks = "3 months",
                   limits = c(as_date("2018-09-01"), as_date("2021-12-01")
    )) + 
  ggrepel::geom_text_repel(
    data = . %>% filter(!is.na(crash_date)),
    aes(x = crash_date, 
        y = n + 200, 
        label = name_lab, 
        color = injuries),
    fontface = "bold",
    size = 4,
    direction = "y",
    xlim = c(2022, NA),
    hjust = 0,
    segment.size = .7,
    segment.alpha = .5,
    segment.linetype = "dotted",
    box.padding = .4,
    segment.curvature = -0.1,
    segment.ncp = 3,
    segment.angle = 20
  ) +
  scale_y_continuous(limits = (c(0, NA))) +
  labs(title = "How have the number of crashes changed over time?",
    x = NULL, y = "Number of traffic crashes per week",
    color = "Injuries?", caption = "Data: Chicago Data Portal | Visual: @jim_gruman"
  ) +
  theme(legend.position = "")
```

This is not a balanced dataset, in that the injuries are a small portion of traffic incidents. Let's look at the percentage.

```{r}
crash %>%
  mutate(crash_date = floor_date(crash_date, unit = "week")) %>%
  count(crash_date, injuries) %>%
  filter(
    crash_date != last(crash_date),
    crash_date != first(crash_date)
  ) %>%
  group_by(crash_date) %>%
  mutate(percent_injury = n / sum(n)) %>%
  ungroup() %>%
  filter(injuries == "injuries") %>%
  ggplot(aes(as_date(crash_date), percent_injury)) +
  geom_line(size = 1.5, alpha = 0.7) +
  scale_y_continuous(limits = c(0, NA), 
                     labels = percent_format(accuracy = 1)) +
  scale_x_date(labels = scales::date_format("%Y"),
                   expand = c(0, 0),
                   breaks = seq.Date(as_date("2018-09-01"),
                                     as_date("2021-09-01"),
                                     by = "year"),
                   minor_breaks = "3 months",
                   limits = c(as_date("2018-09-01"), as_date("2021-12-01")
    )) + 
  labs(x = NULL, y = "% of crashes that involve injuries",
       title = "How has the traffic injury rate changed over time?",
       caption = "Data: Chicago Data Portal | Visual: @jim_gruman")
```

```{r}
crash %>%
  mutate(crash_date = wday(crash_date, label = TRUE)) %>%
  count(crash_date, injuries) %>%
  group_by(injuries) %>%
  mutate(percent = n / sum(n)) %>%
  ungroup() %>%
  ggplot(aes(percent, crash_date, fill = injuries)) +
  geom_col(position = "dodge", alpha = 0.8) +
  scale_x_continuous(labels = percent_format()) +
  labs(x = "% of crashes", y = NULL, fill = NULL,
       title = "How does the injury rate change through the week?",
       caption = "Data: Chicago Data Portal | Visual: @jim_gruman")
```

```{r}
crash %>%
  count(first_crash_type, injuries) %>%
  mutate(first_crash_type = fct_reorder(first_crash_type, n)) %>%
  group_by(injuries) %>%
  mutate(percent = n / sum(n)) %>%
  ungroup() %>%
  group_by(first_crash_type) %>%
  filter(sum(n) > 1e4) %>%
  ungroup() %>%
  ggplot(aes(percent, first_crash_type, fill = injuries)) +
  geom_col(position = "dodge", alpha = 0.8) +
  scale_x_continuous(labels = percent_format()) +
  labs(x = "% of crashes", y = NULL, fill = NULL,
       title = "How do injuries vary with first crash type?",
       caption = "Data: Chicago Data Portal | Visual: @jim_gruman")
```

```{r trafficMap, fig.asp=1}
crash %>%
  filter(latitude > 0) %>%
  ggplot(aes(longitude, latitude, color = injuries)) +
  geom_point(size = 0.5, alpha = 0.4) +
  labs(color = NULL) +
  coord_map() +
  guides(col = guide_legend(override.aes = list(size = 3, alpha = 1))) +
  theme(axis.text.x = element_blank(), axis.text.y = element_blank()) +
  labs(x = NULL, y = NULL, fill = NULL,
       title = "Are injuries more likely in different locations?",
       caption = "Data: Chicago Data Portal | Visual: @jim_gruman")
```

This is all the information we will use in building our model to predict which crashes caused injuries.

## Build a Model

Let’s start by splitting our data and creating 10 cross-validation folds.

```{r}
crash_split <- initial_split(crash, strata = injuries)
crash_train <- training(crash_split)
crash_test <- testing(crash_split)

crash_folds <- vfold_cv(crash_train, 
                        v = 10,
                        strata = injuries)
```

Next, let’s create a model.

The feature engineering includes creating date features such as day of the week, handling the high cardinality of weather conditions, contributing cause, etc, and perhaps most importantly, downsampling to account for the class imbalance (injuries are more rare than non-injury-causing crashes).

```{r}
crash_rec <- recipe(injuries ~ ., data = crash_train) %>%
  step_date(crash_date) %>%
  step_rm(crash_date) %>%
  step_other(
    weather_condition,
    first_crash_type,
    trafficway_type,
    prim_contributory_cause,
    other = "OTHER"
  ) %>%
  step_downsample(injuries)

bag_spec <- bag_tree(min_n = 10) %>%
  set_engine("rpart", times = 25) %>%
  set_mode("classification")

crash_wf <- workflow() %>%
  add_recipe(crash_rec) %>%
  add_model(bag_spec)

crash_wf
```
Let’s fit this model to the cross-validation resamples to understand how well it will perform.

```{r}
all_cores <- parallelly::availableCores(omit = 1)
all_cores

future::plan("multisession", workers = all_cores) # on Windows

crash_res <- fit_resamples(
  crash_wf,
  crash_folds,
  control = control_resamples(save_pred = TRUE)
)
```

## Evaluate the Model

What do the results look like?

```{r}
collect_metrics(crash_res)  # metrics on the training set
```

Not bad.

Let’s now fit to the entire training set and evaluate on the testing set.

```{r}
crash_fit <- last_fit(crash_wf, crash_split)
collect_metrics(crash_fit) # metrics on the test set, look for overfitting
```

Spot on.

Which features were most important in predicting an injury?

```{r}
crash_imp <- crash_fit$.workflow[[1]] %>%
  extract_fit_parsnip()

crash_imp$fit$imp %>%
  slice_max(value, n = 10) %>%
  ggplot(aes(value, fct_reorder(term, value))) +
  geom_col(alpha = 0.8) + 
  labs(x = "Variable importance score", y = NULL) +
  theme(panel.grid.major.y = element_blank())
```

How does the ROC curve for the testing data look?

```{r}
collect_predictions(crash_fit) %>%
  roc_curve(injuries, .pred_injuries) %>%
  ggplot(aes(x = 1 - specificity, y = sensitivity)) +
  geom_line(size = 1.5, color = hrbrthemes::ipsum_pal()(1)) +
  geom_abline(
    lty = 2, alpha = 0.5,
    color = "gray50",
    size = 1.2
  ) +
  coord_equal() +
  labs(title = "ROC Curve")
```

## Save the model

```{r}
crash_wf_model <- crash_fit$.workflow[[1]]

# crash_wf_model <- butcher::butcher(crash_fit$.workflow[[1]])
```

This is an object we can make predictions with. For example, is this particular crash predicted to have any injuries?

```{r}
predict(crash_wf_model, crash_test[222, ])

```



