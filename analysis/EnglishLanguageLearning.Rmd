---
title: "Kaggle Feedback Prize - English Language Learning"
author: "Jim Gruman"
date: "October 15, 2022"
output:
  workflowr::wflow_html:
    toc: no
    code_folding: hide
    code_download: true
    df_print: paged
editor_options:
  chunk_output_type: console
---

The Kaggle Challenge presented here works with a dataset that comprises argumentative essays (the ELLIPSE corpus) written by 8th-12th grade English Language Learners (ELLs). The essays have been scored on six measures: **cohesion, syntax, vocabulary, phraseology, grammar,** and **conventions**.

Each measure represents a component of writing proficiency, ranging from 1.0 to 5.0 in increments of 0.5. Our task is to predict the score of each measure by essay.

![](https://storage.googleapis.com/kaggle-competitions/kaggle/38321/logos/header.png)


[This is the rubric](https://docs.google.com/document/d/1PBNshCCbjIF7Hw4L-dwWHKosNVAHS8P3vHYM_EkpCvA/edit) that was used to grade the essays. Two people did the work independently, and then the scores were compared for alignment.

## Preprocessing 

Natural Language Processing techniques offer a wide variety of tools to approach this problem. The Kaggle host is requiring that the model run as a standalone, without internet assistance. They also ask for a parsimonous, explainable model.

We will start with exploring the predictive potential of the text count features, like numbers of words, distinct words, and spaces.

Unsupervised topic grouping categories may be useful for measures like conventions or grammar.  In this case, we will start with [Latent Dirichlet allocation](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) (LDA).

Individual words may have predictive power, but they could be so sparse as to be difficult to separate from the background noise. Consider words like *ain't* and phrases taken from other languages.

Bringing in a sentiment dictionary may add predictive power to some measures, along with helping to count miss-spellings. Word embeddings like Glove or Huggingface could also better characterize meaning. 

## Modeling 

Many developers are tempted to jump into (CNN / LSTM) deep learning, but the number of essays is really pretty small for a deep learning run on their own. Another approach could leverage the pre-trained embeddings in one of the [BERT](https://en.wikipedia.org/wiki/BERT_(language_model))s. The current Kaggle leaderboard is full of them. Even so, the standings will shift in a huge way after the full test set calculations appear because of overfitting and imbalance.

The GloVe pre-trained word vectors provide word embeddings created on existing document corpus, and are provided as a pre-processor using varying numbers of tokens. See [Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation.](https://nlp.stanford.edu/projects/glove/) for details.

I spent a few evenings with the torch/`brulee` approach on `tidymodels`, but discovered that modeling time consumed would be significant and the results were not better than random forests on engineered features with case weights based on inverse proportions of the metric values.

I ultimately settled on the `xgboost` approach here. No doubt it can still overfit on specific words and text attributes, like the number of unique words. 

One last point. I believe that the Essay Scoring is done by humans in a way where the metrics are judged together, and not entirely independently. In other words, low `grammar` and low `cohesion` are related.

```{r}
#| label: pull packages into memory

suppressPackageStartupMessages({
library(tidyverse)
  
library(tidymodels)
library(text2vec) # for topic modeling

library(tidytext)
library(textrecipes)

})

tidymodels::tidymodels_prefer()

theme_set(theme_minimal())

```

Let's read the data from Kaggle's csv's into dataframes. 

```{r}
#| label: read data files, add pre

train_essays_raw <- read_csv(here::here("data","train.csv"),
                         show_col_types = FALSE) 

submit_essays_raw <- read_csv(here::here("data","test.csv"),
                          show_col_types = FALSE) 

outcomes = names(train_essays_raw)[3:8]

dim(train_essays_raw)

```

The essay metrics score distributions resemble ordinal Likert scales. One way to illustrate the counts at each level is this bar chart:

```{r}
#| label: likert

stage1 <- train_essays_raw |>
  select(cohesion:conventions) |>
  pivot_longer(cols = everything(),
               names_to = "metric",
               values_to = "ans") |>
  group_by(ans, metric) |>
  summarize(n = n(),
            .groups = "drop") |>
  group_by(metric) |>
  mutate(per = n / sum(n)) |>
  mutate(
    text = paste0(formatC(
      100 * per, format = "f", digits = 0
    ), "%"),
    cs = cumsum(per),
    offset = sum(per[1:(floor(n() / 2))]) + (n() %% 2) * 0.5 * (per[ceiling(n() /
                                                                              2)]),
    xmax = -offset + cs,
    xmin = xmax - per
  ) |>
  ungroup()

gap <- 0.2

stage2 <- stage1 %>%
  left_join(
    stage1 %>%
      group_by(metric) %>%
      summarize(max.xmax = max(xmax)) %>%
      mutate(r = row_number(max.xmax)),
    by = "metric"
  ) %>%
  arrange(desc(r)) %>%
  mutate(ymin = r - (1 - gap) / 2,
         ymax = r + (1 - gap) / 2)

ggplot(stage2) +
  geom_vline(xintercept = 0) +
  geom_rect(aes(
    xmin = xmin,
    xmax = xmax,
    ymin = ymin,
    ymax = ymax,
    fill = factor(ans)
  )) +
  geom_text(aes(
    x = (xmin + xmax) / 2,
    y = (ymin + ymax) / 2,
    label = text
  ),
  size = 3,
  check_overlap = TRUE) +
  scale_x_continuous(
    "",
    labels = percent,
    breaks = seq(-0.6, 0.65, len = 6),
    limits = c(-0.6, 0.65)
  ) +   scale_y_continuous(
    "",
    breaks = 1:n_distinct(stage2$metric),
    labels = rev(stage2 %>% distinct(metric) %>% .$metric)
  ) +
  scale_fill_brewer("Score", palette = "BrBG") +
  labs(title = "Training set Essay Ratings")

```

Essays with more words, or more sentences, do not necessarily score better. 

```{r}
#| label: outcome variable distributions

te_long <- train_essays_raw |>
  pivot_longer(cols = cohesion:conventions,
               names_to = "metric",
               values_to = "value") |>
  mutate(metric = as.factor(metric),
         value = as.factor(value))

te_long |> 
  group_by(n_words = ggplot2::cut_interval(
    tokenizers::count_words(full_text), 
    length = 200),
    metric, value) |> 
  summarise(`Number of essays` = n(),
            .groups = "drop") |> 
  ggplot(aes(n_words, `Number of essays`, fill = as.factor(value))) +
  geom_col() +
  scale_x_discrete(guide = guide_axis(n.dodge = 2)) +
  facet_wrap(vars(metric)) +
  scale_fill_brewer("Score", palette = "BrBG") +
  labs(x = "Number of words per essay",
       y = "Number of essays",
       fill = "Score")

te_long |> 
  group_by(n_words = ggplot2::cut_interval(
    tokenizers::count_sentences(full_text), length = 20),
    metric, value) |> 
  summarise(`Number of essays` = n(),
            .groups = "drop") |> 
  ggplot(aes(n_words, `Number of essays`, fill = as.factor(value))) +
  geom_col() +
  scale_x_discrete(guide = guide_axis(n.dodge = 2)) +
  facet_wrap(vars(metric)) +
  scale_fill_brewer("Score", palette = "BrBG") +
  labs(x = "Number of sentences per essay",
       y = "Number of essays",
       fill = "Score")

```

What words from the dialogue have the highest log odds of coming from each level of each outcome?  Do the individual words have predictive power?

```{r}
#| label: log odds

plot_log_odds <- function(outcome = "cohesion"){

train_essays_raw |>
  tidytext::unnest_tokens(word, full_text) |> 
  count(level = factor(.data[[outcome]]), word, sort = TRUE) |>   
  tidylo::bind_log_odds(level, word, n) |> 
  filter(n > 20) |> 
  group_by(level) |> 
  slice_max(log_odds_weighted, n = 10) |> 
  mutate(word = reorder_within(word, log_odds_weighted, level)) %>%
  ggplot(aes(log_odds_weighted, word, fill = level)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(vars(level), scales = "free") +
  scale_fill_brewer("Score", palette = "BrBG") +
  scale_y_reordered() +
  labs(y = NULL, title = glue::glue("{outcome} log odds words"))  
    
}

map(outcomes, plot_log_odds)

```

To some extent, the answer may be yes.

Let's also take a look at outcome pairwise correlations.

```{r}
#| label: predictor and outcome pairwise correlations

train_essays_raw |> 
  corrr::correlate(
    quiet = TRUE
  ) %>%
  corrr::rearrange() %>%
  corrr::shave() %>%
  corrr::rplot(print_cor = TRUE,
               colors = brewer_pal(palette = "BrBG")(5)) +
  scale_x_discrete(guide = guide_axis(n.dodge = 3))

```

*   Vocabulary and Phraseology (0.74) track together.
*   Phraseology and Syntax (0.73) track together.
*   Praseology and Grammar (0.72) track together.

Avoiding overfitting to the training data is critical to achieving a strong score. We are going to use resampling to have some indication that our model generalizes to new essays. Care must be exercised to be sure that members of the hold out folds are not also found in the training folds.

Latent Dirichlet allocation (LDA) is an unsupervised generative statistical model that explains a set of observations through unobserved groups, and the content of each group may explain why some parts of the data are similar.

I'd like to explore the use of `inverse probability weights` because there are so few essays with scores at the highest and lowest levels. When survey respondents have different probabilities of selection, (inverse) probability weights help reduce bias in the results.

I am making us of metaprogramming techniques to pass text vector column names into the formula and case weights functions to re-use them for each metric.


```{r}
#| label: preprocessors


tokens = text2vec::word_tokenizer(tolower(train_essays_raw$full_text))

it = text2vec::itoken(tokens, ids = train_essays_raw$text_id, progressbar = FALSE)

v = text2vec::create_vocabulary(it)

dtm = text2vec::create_dtm(it, text2vec::vocab_vectorizer(v), type = "RsparseMatrix")

lda_model <- text2vec::LDA$new(n_topics = 30)

case_weight_builder <- function(data, outcome) {
  data %>%
    inner_join(data %>%
                 count(.data[[outcome]],
                       name = "case_wts"),
               by = glue::glue("{ outcome }")) %>%
    mutate(case_wts = importance_weights(max(case_wts) / case_wts))
}

recipe_builder <- function(outcome = "cohesion") {
  rec <- recipe(
    formula(glue::glue("{ outcome } ~ .")),
    data = train_essays_raw |>
      select({
        {
          outcome
        }
      }, full_text) |>
      case_weight_builder(outcome)
  ) |>
    step_textfeature(full_text,
                     keep_original_cols = TRUE) |>
    step_rename_at(starts_with("textfeature_"),
                   fn = ~ gsub("textfeature_full_text_", "", .)) %>%
    step_tokenize(full_text) %>%
    step_lda(full_text,
             lda_models = lda_model,
             keep_original_cols = TRUE) %>%
    step_word_embeddings(
      full_text,
      aggregation = "sum",
      embeddings = textdata::embedding_glove27b(dimensions = 200)
    ) |>
    step_zv(all_numeric_predictors()) |>
    step_normalize(all_numeric_predictors())
  
  return(rec)
  
}


multiclass_recipe_builder <- function(outcome = "cohesion") {
  rec <- recipe(formula(glue::glue("{ outcome } ~ .")),
                data = classification_train_df) |>
    step_textfeature(full_text,
                     keep_original_cols = TRUE) |>
    step_rename_at(starts_with("textfeature_"),
                   fn = ~ gsub("textfeature_full_text_", "", .)) %>%
    step_tokenize(full_text) %>%
    step_lda(full_text,
             lda_models = lda_model,
             keep_original_cols = TRUE) %>%
    step_word_embeddings(
      full_text,
      aggregation = "sum",
      embeddings = textdata::embedding_glove27b(dimensions = 200)
    ) |>
    step_zv(all_numeric_predictors()) |>
    step_normalize(all_numeric_predictors())
  
  return(rec)
  
}

plot_preds <- function(dat, outcome){

dat |> 
  ggplot(aes(x = {{outcome}}, y = .pred)) +
  geom_point(alpha = 0.15) +
  geom_abline(color = "red") +
  coord_obs_pred() 

}

```

As mentioned above, the model specification is `xgboost` for regression to predict a continuous outcome that resembles ordinal classes. 

```{r}
#| label: model specification

xgb_spec <-
  boost_tree(
    mtry = 50,  # 75L
    trees = 1000L,
    tree_depth = 9, # 6L
    learn_rate = 0.01,  # originally 0.1
    min_n = 39L,  # 20L
    loss_reduction = 0
  ) |> 
  set_engine('xgboost') |> 
  set_mode('regression')

svm_spec <- svm_linear() |> 
  set_engine("LiblineaR") |> 
  set_mode("classification")  
  

```

To speed the computations let's enable a parallel backend.  

```{r}
#| label: parallel and tuning setup

all_cores <- parallelly::availableCores(omit = 1)
all_cores
#  
future::plan("multisession", workers = all_cores) # on Windows

```

## Modeling {.tabset}

### Cohesion

We fit for `cohesion` first using an xgboost regression, using case weights to adjust for the frequency of occurrence of each value of `cohesion`.  

```{r}
#| label: fit cohesion resamples regression

outcome <- outcomes[1]

regression_train_df <- train_essays_raw  |> 
                select(!!outcome, full_text) |> 
                case_weight_builder(outcome)

regression_wf <- workflow(recipe_builder(outcome = outcome), xgb_spec) |> 
       add_case_weights(case_wts)

folds <- vfold_cv(regression_train_df, strata = {{outcome}})

set.seed(42)  
rs <- fit_resamples(
  regression_wf,
  folds,
  control = control_resamples(save_pred = TRUE))

collect_metrics(rs) |> arrange(mean)

collect_predictions(rs) |> 
  plot_preds(.data[[outcome]]) +
  labs(y = "Predicted",
       title = paste0(outcome, " predictions against essays in held out folds"),
       subtitle = "The highest and lowest essays are not predicted well")

train_essays_raw[

collect_predictions(rs) |> 
  mutate(residual = .data[[outcome]] - .pred) |> 
  arrange(desc(abs(residual))) |> 
  slice_head(n = 5) |> 
  pull(.row)

, ] |> 
  select(full_text)

regression_fit <- parsnip::fit(regression_wf, 
                     regression_train_df)

```

Identifying examples with especially poor performance can help us follow up and investigate why these specific predictions are poor. Conceptually, its easy for a baseline know-nothing model to assign all essays to the median score of 3. The predictive power is in the ability to model the essays that are not 3 into buckets higher and lower than 3.

Because the ratings are a form of ordinal value, or even a likert scale, we will ensemble a second classification model that includes the output of the regression.

```{r}
#| label: fit cohesion resamples classification

classification_train_df <- train_essays_raw  |> 
                select({{outcome}}, full_text) |> 
                bind_cols(
                  predict(
                    regression_fit,
                    regression_train_df
                  )
                ) |> 
               rename(regression_pred = .pred) |> 
               mutate({{outcome}} := factor(.data[[outcome]]))

classification_wf <- workflow(multiclass_recipe_builder(outcome = outcome), svm_spec) 

folds <- vfold_cv(classification_train_df, strata = !!outcome)

set.seed(42)  
rs <- fit_resamples(
  classification_wf,
  folds,
  metrics = metric_set(kap, accuracy),
  control = control_resamples(save_pred = TRUE))

collect_metrics(rs) |> arrange(mean)

collect_predictions(rs) |> 
  ggplot(aes(x = .data[[outcome]], y = abs(as.numeric(.data[[outcome]]) - as.numeric(.pred_class))/2)) +
  geom_violin() +
  scale_y_continuous(breaks = seq(-5,5,0.5)) +
  labs(y = "Residuals",
       title = "{{outcome}} Residual errors for essays in held out folds",
       subtitle = "The highest and lowest essays are still not predicted well")

train_essays_raw[

collect_predictions(rs) |> 
  mutate(residual = as.numeric(.data[[outcome]]) - as.numeric(.pred_class)) |> 
  arrange(desc(abs(residual))) |> 
  slice_head(n = 5) |> 
  pull(.row)

, ] |> 
  select(full_text, {{outcome}})

collect_predictions(rs) |> 
  rmse(truth = as.numeric(.data[[outcome]])/2, estimate = as.numeric(.pred_class)/2)

```

Results here aren't great, but they are more are less competitive with the leaderboard figures.

The final fitting ensembles both the regression and classification fits, and makes a prediction on the submission essays.

```{r}
#| label: fit cohesion final

classification_fit <- parsnip::fit(classification_wf,
                                   classification_train_df)

extract_fit_engine(regression_fit) |> 
  vip::vip(num_features = 20)

submission <- predict(
  classification_fit,
  
  submit_essays_raw |>
    bind_cols(predict(regression_fit, submit_essays_raw)) |>
    rename(regression_pred = .pred)
) |>
  transmute({{outcome}} := .pred_class)

submission

```

### Syntax

We fit for `syntax` first using an xgboost regression, using case weights to adjust for the frequency of occurrence of each value of `syntax`.  

```{r}
#| label: fit syntax resamples regression

outcome <- outcomes[2]

regression_train_df <- train_essays_raw  |> 
                select(!!outcome, full_text) |> 
                case_weight_builder(outcome)

regression_wf <- workflow(recipe_builder(outcome = outcome), xgb_spec) |> 
       add_case_weights(case_wts)

folds <- vfold_cv(regression_train_df, strata = {{outcome}})

set.seed(42)  
rs <- fit_resamples(
  regression_wf,
  folds,
  control = control_resamples(save_pred = TRUE))

collect_metrics(rs) |> arrange(mean)

collect_predictions(rs) |> 
  plot_preds(.data[[outcome]]) +
  labs(y = "Predicted",
       title = paste0(outcome, " predictions against essays in held out folds"),
       subtitle = "The highest and lowest essays are not predicted well")


train_essays_raw[

collect_predictions(rs) |> 
  mutate(residual = .data[[outcome]] - .pred) |> 
  arrange(desc(abs(residual))) |> 
  slice_head(n = 5) |> 
  pull(.row)

, ] |> 
  select(full_text)

regression_fit <- parsnip::fit(regression_wf, 
                     regression_train_df)

```

Identifying examples with especially poor performance can help us follow up and investigate why these specific predictions are poor. Conceptually, its easy for a baseline know-nothing model to assign all essays to the median score of 3. The predictive power is in the ability to model the essays that are not 3 into buckets higher and lower than 3.

Because the ratings are a form of ordinal value, or even a likert scale, we will ensemble a second classification model that includes the output of the regression.

```{r}
#| label: fit syntax resamples classification

classification_train_df <- train_essays_raw  |> 
                select({{outcome}}, full_text) |> 
                bind_cols(
                  predict(
                    regression_fit,
                    regression_train_df
                  )
                ) |> 
               rename(regression_pred = .pred) |> 
               mutate({{outcome}} := factor(.data[[outcome]]))

classification_wf <- workflow(multiclass_recipe_builder(outcome = outcome), svm_spec) 

folds <- vfold_cv(classification_train_df, strata = !!outcome)

set.seed(42)  
rs <- fit_resamples(
  classification_wf,
  folds,
  metrics = metric_set(kap, accuracy),
  control = control_resamples(save_pred = TRUE))

collect_metrics(rs) |> arrange(mean)

collect_predictions(rs) |> 
  ggplot(aes(x = .data[[outcome]], y = abs(as.numeric(.data[[outcome]]) - as.numeric(.pred_class))/2)) +
  geom_violin() +
  scale_y_continuous(breaks = seq(-5,5,0.5)) +
  labs(y = "Residuals",
       title = "{{outcome}} Residual errors for essays in held out folds",
       subtitle = "The highest and lowest essays are still not predicted well")

train_essays_raw[

collect_predictions(rs) |> 
  mutate(residual = as.numeric(.data[[outcome]]) - as.numeric(.pred_class)) |> 
  arrange(desc(abs(residual))) |> 
  slice_head(n = 5) |> 
  pull(.row)

, ] |> 
  select(full_text, {{outcome}})

collect_predictions(rs) |> 
  rmse(truth = as.numeric(.data[[outcome]])/2, estimate = as.numeric(.pred_class)/2)


```

Results here aren't great, but they are more are less competitive with the leaderboard figures.

The final fitting ensembles both the regression and classification fits, and makes a prediction on the submission essays.

```{r}
#| label: fit syntax final

classification_fit <- parsnip::fit(classification_wf,
                                   classification_train_df)

extract_fit_engine(regression_fit) |> 
  vip::vip(num_features = 20)

submission <- predict(
  classification_fit,
  
  submit_essays_raw |>
    bind_cols(predict(regression_fit, submit_essays_raw)) |>
    rename(regression_pred = .pred)
) |>
  transmute({{outcome}} := .pred_class) |>
  bind_cols(submission)

submission

```

### Vocabulary

We fit for `vocabulary` first using an xgboost regression, using case weights to adjust for the frequency of occurrence of each value of `vocabulary`.  

```{r}
#| label: fit vocabulary resamples regression

outcome <- outcomes[3]

regression_train_df <- train_essays_raw  |> 
                select(!!outcome, full_text) |> 
                case_weight_builder(outcome)

regression_wf <- workflow(recipe_builder(outcome = outcome), xgb_spec) |> 
       add_case_weights(case_wts)

folds <- vfold_cv(regression_train_df, strata = {{outcome}})

set.seed(42)  
rs <- fit_resamples(
  regression_wf,
  folds,
  control = control_resamples(save_pred = TRUE))

collect_metrics(rs) |> arrange(mean)

collect_predictions(rs) |> 
  plot_preds(.data[[outcome]]) +
  labs(y = "Predicted",
       title = paste0(outcome, " predictions against essays in held out folds"),
       subtitle = "The highest and lowest essays are not predicted well")


train_essays_raw[

collect_predictions(rs) |> 
  mutate(residual = .data[[outcome]] - .pred) |> 
  arrange(desc(abs(residual))) |> 
  slice_head(n = 5) |> 
  pull(.row)

, ] |> 
  select(full_text)

regression_fit <- parsnip::fit(regression_wf, 
                     regression_train_df)

```

Identifying examples with especially poor performance can help us follow up and investigate why these specific predictions are poor. Conceptually, its easy for a baseline know-nothing model to assign all essays to the median score of 3. The predictive power is in the ability to model the essays that are not 3 into buckets higher and lower than 3.

Because the ratings are a form of ordinal value, or even a likert scale, we will ensemble a second classification model that includes the output of the regression.

```{r}
#| label: fit vocabulary resamples classification

classification_train_df <- train_essays_raw  |> 
                select({{outcome}}, full_text) |> 
                bind_cols(
                  predict(
                    regression_fit,
                    regression_train_df
                  )
                ) |> 
               rename(regression_pred = .pred) |> 
               mutate({{outcome}} := factor(.data[[outcome]]))

classification_wf <- workflow(multiclass_recipe_builder(outcome = outcome), svm_spec) 

folds <- vfold_cv(classification_train_df, strata = !!outcome)

set.seed(42)  
rs <- fit_resamples(
  classification_wf,
  folds,
  metrics = metric_set(kap, accuracy),
  control = control_resamples(save_pred = TRUE))

collect_metrics(rs) |> arrange(mean)

collect_predictions(rs) |> 
  ggplot(aes(x = .data[[outcome]], y = abs(as.numeric(.data[[outcome]]) - as.numeric(.pred_class))/2)) +
  geom_violin() +
  scale_y_continuous(breaks = seq(-5,5,0.5)) +
  labs(y = "Residuals",
       title = "{{outcome}} Residual errors for essays in held out folds",
       subtitle = "The highest and lowest essays are still not predicted well")

train_essays_raw[

collect_predictions(rs) |> 
  mutate(residual = as.numeric(.data[[outcome]]) - as.numeric(.pred_class)) |> 
  arrange(desc(abs(residual))) |> 
  slice_head(n = 5) |> 
  pull(.row)

, ] |> 
  select(full_text, {{outcome}})

collect_predictions(rs) |> 
  rmse(truth = as.numeric(.data[[outcome]])/2, estimate = as.numeric(.pred_class)/2)


```

Results here aren't great, but they are more are less competitive with the leaderboard figures.

The final fitting ensembles both the regression and classification fits, and makes a prediction on the submission essays.

```{r}
#| label: fit vocuabulary final

classification_fit <- parsnip::fit(classification_wf,
                                   classification_train_df)

extract_fit_engine(regression_fit) |> 
  vip::vip(num_features = 20)

submission <- predict(
  classification_fit,
  
  submit_essays_raw |>
    bind_cols(predict(regression_fit, submit_essays_raw)) |>
    rename(regression_pred = .pred)
) |>
  transmute({{outcome}} := .pred_class) |>
  bind_cols(submission)

submission

```

### Phraseology

We fit for `phraseology` first using an xgboost regression, using case weights to adjust for the frequency of occurrence of each value of `phraseology`.  

```{r}
#| label: fit phraseology resamples regression

outcome <- outcomes[4]

regression_train_df <- train_essays_raw  |> 
                select(!!outcome, full_text) |> 
                case_weight_builder(outcome)

regression_wf <- workflow(recipe_builder(outcome = outcome), xgb_spec) |> 
       add_case_weights(case_wts)

folds <- vfold_cv(regression_train_df, strata = {{outcome}})

set.seed(42)  
rs <- fit_resamples(
  regression_wf,
  folds,
  control = control_resamples(save_pred = TRUE))

collect_metrics(rs) |> arrange(mean)

collect_predictions(rs) |> 
  plot_preds(.data[[outcome]]) +
  labs(y = "Predicted",
       title = paste0(outcome, " predictions against essays in held out folds"),
       subtitle = "The highest and lowest essays are not predicted well")


train_essays_raw[

collect_predictions(rs) |> 
  mutate(residual = .data[[outcome]] - .pred) |> 
  arrange(desc(abs(residual))) |> 
  slice_head(n = 5) |> 
  pull(.row)

, ] |> 
  select(full_text)

regression_fit <- parsnip::fit(regression_wf, 
                     regression_train_df)

```

Identifying examples with especially poor performance can help us follow up and investigate why these specific predictions are poor. Conceptually, its easy for a baseline know-nothing model to assign all essays to the median score of 3. The predictive power is in the ability to model the essays that are not 3 into buckets higher and lower than 3.

Because the ratings are a form of ordinal value, or even a likert scale, we will ensemble a second classification model that includes the output of the regression.

```{r}
#| label: fit phraseology resamples classification

classification_train_df <- train_essays_raw  |> 
                select({{outcome}}, full_text) |> 
                bind_cols(
                  predict(
                    regression_fit,
                    regression_train_df
                  )
                ) |> 
               rename(regression_pred = .pred) |> 
               mutate({{outcome}} := factor(.data[[outcome]]))

classification_wf <- workflow(multiclass_recipe_builder(outcome = outcome), svm_spec) 

folds <- vfold_cv(classification_train_df, strata = !!outcome)

set.seed(42)  
rs <- fit_resamples(
  classification_wf,
  folds,
  metrics = metric_set(kap, accuracy),
  control = control_resamples(save_pred = TRUE))

collect_metrics(rs) |> arrange(mean)

collect_predictions(rs) |> 
  ggplot(aes(x = .data[[outcome]], y = abs(as.numeric(.data[[outcome]]) - as.numeric(.pred_class))/2)) +
  geom_violin() +
  scale_y_continuous(breaks = seq(-5,5,0.5)) +
  labs(y = "Residuals",
       title = "{{outcome}} Residual errors for essays in held out folds",
       subtitle = "The highest and lowest essays are still not predicted well")

train_essays_raw[

collect_predictions(rs) |> 
  mutate(residual = as.numeric(.data[[outcome]]) - as.numeric(.pred_class)) |> 
  arrange(desc(abs(residual))) |> 
  slice_head(n = 5) |> 
  pull(.row)

, ] |> 
  select(full_text, {{outcome}})

collect_predictions(rs) |> 
  rmse(truth = as.numeric(.data[[outcome]])/2, estimate = as.numeric(.pred_class)/2)


```

Results here aren't great, but they are more are less competitive with the leaderboard figures.

The final fitting ensembles both the regression and classification fits, and makes a prediction on the submission essays.

```{r}
#| label: fit phraseology final

classification_fit <- parsnip::fit(classification_wf,
                                   classification_train_df)

extract_fit_engine(regression_fit) |> 
  vip::vip(num_features = 20)

submission <- predict(
  classification_fit,
  
  submit_essays_raw |>
    bind_cols(predict(regression_fit, submit_essays_raw)) |>
    rename(regression_pred = .pred)
) |>
  transmute({{outcome}} := .pred_class) |>
  bind_cols(submission)

submission

```

### Grammar

We fit for `grammar` first using an xgboost regression, using case weights to adjust for the frequency of occurrence of each value of `grammar`.  

```{r}
#| label: fit grammar resamples regression

outcome <- outcomes[5]

regression_train_df <- train_essays_raw  |> 
                select(!!outcome, full_text) |> 
                case_weight_builder(outcome)

regression_wf <- workflow(recipe_builder(outcome = outcome), xgb_spec) |> 
       add_case_weights(case_wts)

folds <- vfold_cv(regression_train_df, strata = {{outcome}})

set.seed(42)  
rs <- fit_resamples(
  regression_wf,
  folds,
  control = control_resamples(save_pred = TRUE))

collect_metrics(rs) |> arrange(mean)

collect_predictions(rs) |> 
  plot_preds(.data[[outcome]]) +
  labs(y = "Predicted",
       title = paste0(outcome, " predictions against essays in held out folds"),
       subtitle = "The highest and lowest essays are not predicted well")


train_essays_raw[

collect_predictions(rs) |> 
  mutate(residual = .data[[outcome]] - .pred) |> 
  arrange(desc(abs(residual))) |> 
  slice_head(n = 5) |> 
  pull(.row)

, ] |> 
  select(full_text)

regression_fit <- parsnip::fit(regression_wf, 
                     regression_train_df)

```

Identifying examples with especially poor performance can help us follow up and investigate why these specific predictions are poor. Conceptually, its easy for a baseline know-nothing model to assign all essays to the median score of 3. The predictive power is in the ability to model the essays that are not 3 into buckets higher and lower than 3.

Because the ratings are a form of ordinal value, or even a likert scale, we will ensemble a second classification model that includes the output of the regression.

```{r}
#| label: fit grammar resamples classification

classification_train_df <- train_essays_raw  |> 
                select({{outcome}}, full_text) |> 
                bind_cols(
                  predict(
                    regression_fit,
                    regression_train_df
                  )
                ) |> 
               rename(regression_pred = .pred) |> 
               mutate({{outcome}} := factor(.data[[outcome]]))

classification_wf <- workflow(multiclass_recipe_builder(outcome = outcome), svm_spec) 

folds <- vfold_cv(classification_train_df, strata = !!outcome)

set.seed(42)  
rs <- fit_resamples(
  classification_wf,
  folds,
  metrics = metric_set(kap, accuracy),
  control = control_resamples(save_pred = TRUE))

collect_metrics(rs) |> arrange(mean)

collect_predictions(rs) |> 
  ggplot(aes(x = .data[[outcome]], y = abs(as.numeric(.data[[outcome]]) - as.numeric(.pred_class))/2)) +
  geom_violin() +
  scale_y_continuous(breaks = seq(-5,5,0.5)) +
  labs(y = "Residuals",
       title = "{{outcome}} Residual errors for essays in held out folds",
       subtitle = "The highest and lowest essays are still not predicted well")

train_essays_raw[

collect_predictions(rs) |> 
  mutate(residual = as.numeric(.data[[outcome]]) - as.numeric(.pred_class)) |> 
  arrange(desc(abs(residual))) |> 
  slice_head(n = 5) |> 
  pull(.row)

, ] |> 
  select(full_text, {{outcome}})

collect_predictions(rs) |> 
  rmse(truth = as.numeric(.data[[outcome]])/2, estimate = as.numeric(.pred_class)/2)

```

Results here aren't great, but they are more are less competitive with the leaderboard figures.

The final fitting ensembles both the regression and classification fits, and makes a prediction on the submission essays.

```{r}
#| label: fit grammar final

classification_fit <- parsnip::fit(classification_wf,
                                   classification_train_df)

extract_fit_engine(regression_fit) |> 
  vip::vip(num_features = 20)

submission <- predict(
  classification_fit,
  
  submit_essays_raw |>
    bind_cols(predict(regression_fit, submit_essays_raw)) |>
    rename(regression_pred = .pred)
) |>
  transmute({{outcome}} := .pred_class) |>
  bind_cols(submission)

submission

```

### Conventions

We fit for `conventions` first using an xgboost regression, using case weights to adjust for the frequency of occurrence of each value of `conventions`.  

```{r}
#| label: fit conventions resamples regression

outcome <- outcomes[6]

regression_train_df <- train_essays_raw  |> 
                select(!!outcome, full_text) |> 
                case_weight_builder(outcome)

regression_wf <- workflow(recipe_builder(outcome = outcome), xgb_spec) |> 
       add_case_weights(case_wts)

folds <- vfold_cv(regression_train_df, strata = {{outcome}})

set.seed(42)  
rs <- fit_resamples(
  regression_wf,
  folds,
  control = control_resamples(save_pred = TRUE))

collect_metrics(rs) |> arrange(mean)

collect_predictions(rs) |> 
  plot_preds(.data[[outcome]]) +
  labs(y = "Predicted",
       title = paste0(outcome, " predictions against essays in held out folds"),
       subtitle = "The highest and lowest essays are not predicted well")


train_essays_raw[

collect_predictions(rs) |> 
  mutate(residual = .data[[outcome]] - .pred) |> 
  arrange(desc(abs(residual))) |> 
  slice_head(n = 5) |> 
  pull(.row)

, ] |> 
  select(full_text)

regression_fit <- parsnip::fit(regression_wf, 
                     regression_train_df)

```

Identifying examples with especially poor performance can help us follow up and investigate why these specific predictions are poor. Conceptually, its easy for a baseline know-nothing model to assign all essays to the median score of 3. The predictive power is in the ability to model the essays that are not 3 into buckets higher and lower than 3.

Because the ratings are a form of ordinal value, or even a likert scale, we will ensemble a second classification model that includes the output of the regression.

```{r}
#| label: fit conventions resamples classification

classification_train_df <- train_essays_raw  |> 
                select({{outcome}}, full_text) |> 
                bind_cols(
                  predict(
                    regression_fit,
                    regression_train_df
                  )
                ) |> 
               rename(regression_pred = .pred) |> 
               mutate({{outcome}} := factor(.data[[outcome]]))

classification_wf <- workflow(multiclass_recipe_builder(outcome = outcome), svm_spec) 

folds <- vfold_cv(classification_train_df, strata = !!outcome)

set.seed(42)  
rs <- fit_resamples(
  classification_wf,
  folds,
  metrics = metric_set(kap, accuracy),
  control = control_resamples(save_pred = TRUE))

collect_metrics(rs) |> arrange(mean)

collect_predictions(rs) |> 
  ggplot(aes(x = .data[[outcome]], y = abs(as.numeric(.data[[outcome]]) - as.numeric(.pred_class))/2)) +
  geom_violin() +
  scale_y_continuous(breaks = seq(-5,5,0.5)) +
  labs(y = "Residuals",
       title = "{{outcome}} Residual errors for essays in held out folds",
       subtitle = "The highest and lowest essays are still not predicted well")

train_essays_raw[

collect_predictions(rs) |> 
  mutate(residual = as.numeric(.data[[outcome]]) - as.numeric(.pred_class)) |> 
  arrange(desc(abs(residual))) |> 
  slice_head(n = 5) |> 
  pull(.row)

, ] |> 
  select(full_text, {{outcome}})

collect_predictions(rs) |> 
  rmse(truth = as.numeric(.data[[outcome]])/2, estimate = as.numeric(.pred_class)/2)


```

Results here aren't great, but they are more are less competitive with the leaderboard figures.

The final fitting ensembles both the regression and classification fits, and makes a prediction on the submission essays.

```{r}
#| label: fit conventions final

classification_fit <- parsnip::fit(classification_wf,
                                   classification_train_df)

extract_fit_engine(regression_fit) |> 
  vip::vip(num_features = 20)

submission <- predict(
  classification_fit,
  
  submit_essays_raw |>
    bind_cols(predict(regression_fit, submit_essays_raw)) |>
    rename(regression_pred = .pred)
) |>
  transmute({{outcome}} := .pred_class) |>
  bind_cols(submission)

submission

```

## {-}

# The Submission

Kaggle's system runs the workbook twice. The first time is on the tiny three line public test dataset here. The second time is on a much much larger hidden test dataset.  As a check to simulate how the hidden datset might fit, we could re-fit on the train dataset text across all of the fits.

```{r}
#| label: write submission out as a csv

submission

# write_csv(submission, here::here("data", "submission.csv"))
```

# Outcome

Not only was this exercise a good study of Likert evaluation data, but also of NLP techniques and of statistical resampling to assure that the model performs on unseen data.  The resulting models here lack the predictive power needed for production use. 







